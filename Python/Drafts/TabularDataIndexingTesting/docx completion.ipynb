{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53c2670",
   "metadata": {},
   "source": [
    "## Loading CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93039020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047db067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads csv file\n",
    "def get_data(file_path):\n",
    "    loader = CSVLoader(file_path)\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ba7843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "\n",
      "\n",
      "page_content='\\ufeffMonthly Population Served: 25 to 1000\\nService Connections: 15 to 400\\nMinimum Number of Samples Per Month: 1\\n: ' metadata={'source': './Data/Bacti Samples Table from dw_regulations_2021_0701_effective_RTCR.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "data = get_data(file_path='./Data/Bacti Samples Table from dw_regulations_2021_0701_effective_RTCR.csv')\n",
    "print(type(data))\n",
    "print(\"\\n\")\n",
    "print(data[0])\n",
    "\n",
    "# TODOS\n",
    "# 1. redo? macros for csvs to format in same format as \"Bacti Samples Table from dw_regulations_2021_0701_effective_RTCR.csv\"\n",
    "# 2. TEST relevant string data with tabular data using OpenAI querying/completion - use 1.csv and relevant info as test\n",
    "# 3. store in dict, figure out how to add strings to an updated docx file (replace prev paragraphs? or just add idk)\n",
    "# 4. reuse this docx file to re-index - can use previous files once docx is in Data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236682c",
   "metadata": {},
   "source": [
    "## Getting Relevant Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1624b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "from docx import Document\n",
    "from docx import table\n",
    "from docx.shared import Inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec979382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUAL TRAVERSAL\n",
    "\n",
    "# Open the document\n",
    "doc = docx.Document('./Data/raw_regulations.docx')\n",
    "\n",
    "# Create an empty list to hold the paragraphs and tables\n",
    "content = []\n",
    "\n",
    "# Iterate through the document's block elements\n",
    "for block in doc.element.body:\n",
    "    if isinstance(block, docx.oxml.text.paragraph.CT_P):\n",
    "        # If the block is a paragraph and has text, append its text to the content list\n",
    "        if (docx.text.paragraph.Paragraph(block, doc).text != \"\"):\n",
    "            content.append(docx.text.paragraph.Paragraph(block, doc))\n",
    "    elif isinstance(block, docx.oxml.table.CT_Tbl):\n",
    "        # If the block is a table, append it to the content list\n",
    "        content.append(docx.table.Table(block, doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dce80b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELEVANT CONTENT\n",
    "# each element of relevant_content array reflects ith table's preceding 3 paragraphs\n",
    "\n",
    "relevant_content = []\n",
    "tables = []\n",
    "\n",
    "def return_relevant_content(i):\n",
    "    relevant_content = \"\"\n",
    "    for j in range(-3, 0):\n",
    "        if not isinstance(content[i+j], docx.table.Table):\n",
    "            relevant_content += content[i+j].text\n",
    "    return relevant_content\n",
    "\n",
    "\n",
    "for i in range(0, len(content)):\n",
    "    item = content[i]\n",
    "    if isinstance(item, docx.table.Table):\n",
    "        relevant_content.append(return_relevant_content(i))\n",
    "        tables.append(item)\n",
    "        # below code is helpful for visualization\n",
    "        # print(\"Paragraph:\", return_relevant_content(i))\n",
    "        # print(\"Table:\", item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29ec22c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This content is relevant to table 1:\n",
      "\n",
      "The type of protection that shall be provided to prevent backflow into the public water supply shall be commensurate with the degree of hazard that exists on the consumer's premises. The type of protective device that may be required (listed in an increasing level of protection) includes: Double check Valve Assembly-(DC), Reduced Pressure Principle Backflow Prevention Device-(RP) and an Air gap Separation-(AG). The water user may choose a higher level of protection than required by the water supplier. The minimum types of backflow protection required to protect the public water supply, at the water user's connection to premises with various degrees of hazard, are given in Table 1. Situations not covered in Table 1 shall be evaluated on a case-by-case basis and the appropriate backflow protection shall be determined by the water supplier or health agency.\n",
      "TABLE 1TYPE OF BACKFLOW PROTECTION REQUIRED\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "print(\"This content is relevant to table 1:\\n\\n\" + relevant_content[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e24ab0",
   "metadata": {},
   "source": [
    "## Generating Docx Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "022b3ea6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in /opt/anaconda3/lib/python3.9/site-packages (0.3.11)\n",
      "Requirement already satisfied: pandas>=1.3 in /opt/anaconda3/lib/python3.9/site-packages (from chromadb) (1.3.4)\n",
      "Requirement already satisfied: requests>=2.28 in /opt/anaconda3/lib/python3.9/site-packages (from chromadb) (2.28.2)\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/anaconda3/lib/python3.9/site-packages (from chromadb) (1.10.6)\n",
      "Requirement already satisfied: hnswlib>=0.7 in /opt/anaconda3/lib/python3.9/site-packages (from chromadb) (0.7.0)\n",
      "Requirement already satisfied: clickhouse-connect>=0.5.7 in /opt/anaconda3/lib/python3.9/site-packages (from chromadb) (0.5.16)\n",
      "Requirement already satisfied: sentence-transformers>=2.2.2 in /opt/anaconda3/lib/python3.9/site-packages (from chromadb) (2.2.2)\n",
      "Requirement already satisfied: duckdb>=0.5.1 in /opt/anaconda3/lib/python3.9/site-packages (from chromadb) (0.7.1)\n",
      "Requirement already satisfied: fastapi>=0.85.1 in /opt/anaconda3/lib/python3.9/site-packages (from chromadb) (0.95.0)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /opt/anaconda3/lib/python3.9/site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /opt/anaconda3/lib/python3.9/site-packages (from chromadb) (1.22.4)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.12.7)\n",
      "Requirement already satisfied: urllib3>=1.26 in /opt/anaconda3/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (1.26.15)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2021.3)\n",
      "Requirement already satisfied: zstandard in /opt/anaconda3/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (0.20.0)\n",
      "Requirement already satisfied: lz4 in /opt/anaconda3/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (4.3.2)\n",
      "Requirement already satisfied: starlette<0.27.0,>=0.26.1 in /opt/anaconda3/lib/python3.9/site-packages (from fastapi>=0.85.1->chromadb) (0.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic>=1.9->chromadb) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.28->chromadb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests>=2.28->chromadb) (2.10)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (4.27.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (2.0.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (0.15.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (0.24.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (1.7.1)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (3.6.5)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (0.1.97)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (0.12.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/anaconda3/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.0.3)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/anaconda3/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.13.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /opt/anaconda3/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.5.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/anaconda3/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (5.4.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/anaconda3/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.18.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (10.4)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (3.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (21.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas>=1.3->chromadb) (1.16.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from starlette<0.27.0,>=0.26.1->fastapi>=0.85.1->chromadb) (3.6.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (1.9)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb) (0.13.2)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.9/site-packages (from nltk->sentence-transformers>=2.2.2->chromadb) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.9/site-packages (from torchvision->sentence-transformers>=2.2.2->chromadb) (8.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi>=0.85.1->chromadb) (1.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (1.2.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Using default OpenAI Model to index and query\n",
    "!pip install chromadb\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.indexes.vectorstore import VectorstoreIndexCreator\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"your key here\" # your key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76471a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
    "\n",
    "# answers question using langchain stuff chain; prints tokens used\n",
    "def answer_question(query):\n",
    "    with get_openai_callback() as cb:\n",
    "        output = chain.run(input_documents=data, question=query)\n",
    "        # print(output)\n",
    "        # print(f\"Total Tokens: {cb.total_tokens}\" + \"\\n\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06192c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this generates context\n",
    "def generate_context(relevant_text):\n",
    "    query = f\"Generate a paragraph describing the meaning of this table, with context being: {relevant_text}\"\n",
    "    return answer_question(query)\n",
    "    \n",
    "# this generates text version\n",
    "def generate_text(file_path):\n",
    "    with get_openai_callback() as cb:\n",
    "        d = get_data(file_path)\n",
    "        query = \"generate text version without loss of information.\"\n",
    "        output = chain.run(input_documents=d, question=query)\n",
    "        print(f\"Total Tokens: {cb.total_tokens}\" + \"\\n\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fac161c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 1244\n",
      "\n",
      "Total Tokens: 302\n",
      "\n",
      "Total Tokens: 219\n",
      "\n",
      "Total Tokens: 228\n",
      "\n",
      "Total Tokens: 186\n",
      "\n",
      "Total Tokens: 324\n",
      "\n",
      "Total Tokens: 382\n",
      "\n",
      "Total Tokens: 308\n",
      "\n",
      "Total Tokens: 254\n",
      "\n",
      "Total Tokens: 167\n",
      "\n",
      "Total Tokens: 880\n",
      "\n",
      "Total Tokens: 210\n",
      "\n",
      "Total Tokens: 188\n",
      "\n",
      "Total Tokens: 256\n",
      "\n",
      "Total Tokens: 156\n",
      "\n",
      "Total Tokens: 170\n",
      "\n",
      "Total Tokens: 187\n",
      "\n",
      "Total Tokens: 188\n",
      "\n",
      "Total Tokens: 170\n",
      "\n",
      "Total Tokens: 191\n",
      "\n",
      "Total Tokens: 219\n",
      "\n",
      "Total Tokens: 1316\n",
      "\n",
      "Total Tokens: 292\n",
      "\n",
      "Total Tokens: 504\n",
      "\n",
      "Total Tokens: 531\n",
      "\n",
      "Total Tokens: 515\n",
      "\n",
      "Total Tokens: 527\n",
      "\n",
      "Total Tokens: 447\n",
      "\n",
      "ERROR AT TABLE 28\n",
      "Total Tokens: 306\n",
      "\n",
      "Total Tokens: 320\n",
      "\n",
      "Total Tokens: 2141\n",
      "\n",
      "ERROR AT TABLE 32\n",
      "ERROR AT TABLE 33\n",
      "Total Tokens: 571\n",
      "\n",
      "Total Tokens: 270\n",
      "\n",
      "Total Tokens: 753\n",
      "\n",
      "Total Tokens: 642\n",
      "\n",
      "Total Tokens: 3318\n",
      "\n",
      "Total Tokens: 449\n",
      "\n",
      "Total Tokens: 461\n",
      "\n",
      "Total Tokens: 769\n",
      "\n",
      "Total Tokens: 202\n",
      "\n",
      "Total Tokens: 441\n",
      "\n",
      "Total Tokens: 1422\n",
      "\n",
      "Total Tokens: 1528\n",
      "\n",
      "Total Tokens: 1723\n",
      "\n",
      "Total Tokens: 1019\n",
      "\n",
      "Total Tokens: 235\n",
      "\n",
      "Total Tokens: 589\n",
      "\n",
      "Total Tokens: 411\n",
      "\n",
      "Total Tokens: 217\n",
      "\n",
      "Total Tokens: 159\n",
      "\n",
      "Total Tokens: 410\n",
      "\n",
      "Total Tokens: 1340\n",
      "\n",
      "Total Tokens: 1435\n",
      "\n",
      "Total Tokens: 1473\n",
      "\n",
      "Total Tokens: 660\n",
      "\n",
      "Total Tokens: 911\n",
      "\n",
      "Total Tokens: 579\n",
      "\n",
      "Total Tokens: 469\n",
      "\n",
      "Total Tokens: 192\n",
      "\n",
      "Total Tokens: 694\n",
      "\n",
      "Total Tokens: 411\n",
      "\n",
      "Total Tokens: 1029\n",
      "\n",
      "Total Tokens: 1559\n",
      "\n",
      "Total Tokens: 1826\n",
      "\n",
      "Total Tokens: 1374\n",
      "\n",
      "Total Tokens: 244\n",
      "\n",
      "Total Tokens: 1102\n",
      "\n",
      "Total Tokens: 678\n",
      "\n",
      "Total Tokens: 435\n",
      "\n",
      "Total Tokens: 737\n",
      "\n",
      "Total Tokens: 329\n",
      "\n",
      "Total Tokens: 470\n",
      "\n",
      "Total Tokens: 228\n",
      "\n",
      "Total Tokens: 1402\n",
      "\n",
      "Total Tokens: 1627\n",
      "\n",
      "Total Tokens: 224\n",
      "\n",
      "Total Tokens: 1525\n",
      "\n",
      "Total Tokens: 973\n",
      "\n",
      "Total Tokens: 1272\n",
      "\n",
      "Total Tokens: 1068\n",
      "\n",
      "Total Tokens: 429\n",
      "\n",
      "Total Tokens: 483\n",
      "\n",
      "Total Tokens: 189\n",
      "\n",
      "Total Tokens: 453\n",
      "\n",
      "Total Tokens: 210\n",
      "\n",
      "Total Tokens: 432\n",
      "\n",
      "Total Tokens: 139\n",
      "\n",
      "Total Tokens: 165\n",
      "\n",
      "Total Tokens: 356\n",
      "\n",
      "Total Tokens: 302\n",
      "\n",
      "Total Tokens: 577\n",
      "\n",
      "ERROR AT TABLE 94\n",
      "Total Tokens: 568\n",
      "\n",
      "Total Tokens: 223\n",
      "\n",
      "Total Tokens: 239\n",
      "\n",
      "Total Tokens: 494\n",
      "\n",
      "Total Tokens: 526\n",
      "\n",
      "Total Tokens: 322\n",
      "\n",
      "Total Tokens: 1491\n",
      "\n",
      "Total Tokens: 713\n",
      "\n",
      "Total Tokens: 753\n",
      "\n",
      "Total Tokens: 539\n",
      "\n",
      "Total Tokens: 1954\n",
      "\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# insert generate_context + generate_text\n",
    "file = open('./Data/txt_with_table_data.txt', 'w')\n",
    "curr_table_index = 0\n",
    "\n",
    "# Iterate through the document's block elements\n",
    "for block in doc.element.body:\n",
    "    if isinstance(block, docx.oxml.text.paragraph.CT_P):\n",
    "        # If the block is a paragraph and has text, write paragraph to txt\n",
    "        if (docx.text.paragraph.Paragraph(block, doc).text != \"\"):\n",
    "            curr_paragraph = docx.text.paragraph.Paragraph(block, doc).text\n",
    "            file.write(curr_paragraph + \"\\n\")\n",
    "    elif isinstance(block, docx.oxml.table.CT_Tbl):\n",
    "        # If the block is a table, add context and table text rep to txt\n",
    "        try:\n",
    "            file.write(generate_context(relevant_content[curr_table_index]) + \"\\n\" + generate_text(f\"./Data/regulation_csv_comma_sep/{curr_table_index}.csv\"))\n",
    "        except:\n",
    "            print(f\"ERROR AT TABLE {curr_table_index}\")\n",
    "            file.write(f\"loss of information for table {curr_table_index}\")\n",
    "        curr_table_index += 1\n",
    "\n",
    "file.close()\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce64e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53342135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752b1f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
